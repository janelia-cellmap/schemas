## Introduction to COSEM
The [COSEM](https://www.janelia.org/project-team/cosem) (Cellular Organelle Segmentation in Electron Microscopy) project team at Janelia Research campus uses computer vision techniques to scalably detect subcellular structures in datasets generated with next-generation volumetric electron microscopy.  

COSEM processes datasets that are acquired by Focused Ion Beam - Scanning Electron Microscopy (FIB-SEM). FIB-SEM is an electron microscopy technique that enables volumetric imaging of single cells and / or bulk tissue at nanometer isotropic resolution. For more information about FIB-SEM microscopy and its applications, see these publications: [Xu et al.](https://elifesciences.org/articles/25916), [Hoffman et al.](https://science.sciencemag.org/content/367/6475/eaaz5357), and these groups at Janelia Research Campus: [Hess lab](https://www.janelia.org/lab/hess-lab), [FIBSEM Technology](https://www.janelia.org/project-team/fib-sem-technology) 

FIB-SEM datasets are large (hundreds of gigabytes), dense, and extremely detailed; these features pose a challenge for image processing routines. COSEM develops machine learning tools that segment features of interest -- ribosomes, mitochondria, the endoplasmic reticulum, and other organelles -- from FIB-SEM datasets. The segmented organelles form a semantic layer on top of the raw FIB-SEM data that biologists can use to answer basic questions about how cells are organized. 

For more about the COSEM project team, see our recent [preprint](https://www.biorxiv.org/content/10.1101/2020.11.14.382143v1).

## Cloud-hosted datasets
Each FIBSEM dataset is stored on [Amazon Web Services](https://aws.amazon.com/) object storage service [S3](https://aws.amazon.com/s3/). Amazon is generously hosting our datasets free of charge. To download data from s3 to a local file system, we recommend the [AWS Command Line Interface](https://aws.amazon.com/cli/). 

### Programmatic access

Most image volumes are stored in the chunked hierarchical array container format [N5](https://github.com/saalfeldlab/n5). Programmers can access to N5 containers through the following libraries: [n5](https://github.com/saalfeldlab/n5) (Java), [zarr](https://github.com/zarr-developers/zarr-python) (Python), [z5](https://github.com/constantinpape/z5) (C++, with python bindings available), and [rust-n5](https://github.com/aschampion/rust-n5) (Rust). Conventionally, all N5 containers begin with a directory suffixed with the string `.n5`, e.g. `jrc_hela-2.n5/`.

A subset of the image volumes are stored with in the [Neuroglancer Precomputed](https://github.com/google/neuroglancer/blob/master/src/neuroglancer/datasource/precomputed/volume.md) format. These volumes can be accessed via [TensorStore](https://github.com/google/tensorstore) (C++/Python) and [CloudVolume](https://github.com/seung-lab/cloud-volume) (Python). Conventionally, all Neuroglancer Precomputed datasets are stored in a directory suffixed with the string `.precomputed`, e.g. `jrc_hela-2.precomputed/`.     

### Visualization 

To browse datasets, we recommend using our data portal [OpenOrganelle](https://www.openorganelle.org), which uses the web-based visualization tool [Neuroglancer](http://neuroglancer-demo.appspot.com/) to display data volumes. One may also visualize our datasets through the [Fiji](https://imagej.net/Fiji) image processing program by using the `n5-ij` [plugin](https://github.com/saalfeldlab/n5-ij)


### Organization of datasets

For each dataset we provide some or all of the following sources of data, all of which should embed in the same physical coordinate space:
* FIB-SEM data: FIB-SEM volumes that have been aligned and minimally preprocessed.

* Light microscopy (LM) data: For some datasets, samples were imaged with light microscopy prior to the acquisition of FIB-SEM images. These LM volumes typically contain multiple channels that each represent a fluorescently labelled organelle or subcellular structure. 

* Ground-truth: Regions of the raw FIB-SEM volume wherein every voxel has been labelled by a human annotator. These annotations are used to train machine learning models. There are typically many ground-truth volumes per FIB-SEM dataset. Within each volume, voxels are assigned a class such as Endoplasmic Reticulum (ER), Mitochondira (mito), Plasma Membrane (PM), etc.

* Predictions: A collection of volumes, one per label class, each of which is a soft segmentation of the FIB-SEM data for a particular object class. These volumes are generated by machine learning models trained on the ground truth data described above. Typically these volumes are a scalar field of `uint8` values that are high at a location inside an instance of an object class and low at a location outside an instance of an object class. 

* Segmentations: A collection of volumes, one per label class, each of which is a putative segmentation of the FIB-SEM data for a particular object class. In these volumes, each instance of a class is given a unique integral ID, which enables analyses like counting organelles or quantifying their relative distances. These volumes are generated by post-processing prediction volumes. 

* As our methods improve we will likely add additional forms of derived data, e.g. mesh representations of segmented organelles.

To facilitate visualization, each volume type (e.g, raw FIB-SEM data) is saved as a collection of arrays ("dataset" in n5 parlance) with decreasing resolutions; `s0` is the dataset with highest resolution, `s1` is half resolution, etc. This structure is schematized with the following tree diagram:  

<pre>
<font color="#34a434"><b> # structure of a multiresolution n5 group </font> </b>
├── <font color="#3465A4"><b>volume_name</b></font> <font color="#34a434"><b> # n5 group </font> </b>
│   ├── <font color="#3465A4"><b>s0</b></font><font color="#34a434"><b> # n5 dataset containing highest resolution data </font> </b>
│   ├── <font color="#3465A4"><b>s1</b></font><font color="#34a434"><b> # n5 dataset containing second-highest resolution data </font> </b>
┊   ┊
│   ├── <font color="#3465A4"><b>sN</b></font><font color="#34a434"><b> # n5 dataset containing lowest resolution data </font> </b>
</pre>

The following tree diagram illustrates how volumetric array data are organized within the s3 bucket for a single logical dataset, which may contain many volumes. Unless otherwise indicated, all the leaves in this diagram are multiresolution volumes as described in the previous diagram. 

<pre>
<font color="#34a434"><b> # organization of datasets in cloud storage </font> </b>
<font color="#3465A4"><b>s3://janelia-cosem</b></font>
├──<font color="#3465A4"><b>jrc_example-dataset-1</b></font>
│  ├── <font color="#3465A4"><b>jrc_example-dataset-1.n5</b></font> <font color="#34a434"><b># root n5 container </b></font>
│  │   ├── <font color="#3465A4"><b>em</b></font> <font color="#34a434"><b># FIB-SEM data </b></font>
│  │   │   ├── <font color="#3465A4"><b>fibsem-uint8</b></font> <font color="#34a434"><b># FIB-SEM data, uint8 datatype </b></font>
┊  ┊   ┊   ┊
│  │   ├── <font color="#3465A4"><b>labels</b></font> <font color="#34a434"><b># raw predictions, refined segmentations, ground truth data </b></font>
│  │   │   ├── <font color="#3465A4"><b>er_pred</b></font> <font color="#34a434"><b># endoplasmic reticulum predictions </b></font>
│  │   │   ├── <font color="#3465A4"><b>er_seg</b></font> <font color="#34a434"><b># endoplasmic reticulum segmentation </b></font>
│  │   │   ├── <font color="#3465A4"><b>gt</b></font> <font color="#34a434"><b># Human-generated ground truth  </b></font>
┊  ┊   ┊   ┊
│  │   └── <font color="#3465A4"><b>lm</b></font> <font color="#34a434"><b># light microscopy data </b></font>
│  │      ├── <font color="#3465A4"><b>er_palm</b></font> <font color="#34a434"><b># PALM microscopy data of the endoplasmic reticulum </b></font>
┊  ┊      ┊
├── <font color="#3465A4"><b>neuroglancer</b></font> <font color="#34a434"><b># neuroglancer precomputed volumes</b></font>
│   └── <font color="#3465A4"><b>em</b></font>
│       └── <font color="#3465A4"><b>fibsem-uint8.precomputed</b></font>
├── index.json <font color="#34a434"><b># JSON manifest of bucket contents</b></font>
├── README.json <font color="#34a434"><b># JSON document containing machine-readable dataset metadata</b></font>
├── README.md <font color="#34a434"><b># markdown document containing human-readable dataset metadata</b></font>
└── thumbnail.jpg <font color="#34a434"><b># thumbnail image for the dataset</b></font>
</pre>

### Ground truth data

COSEM uses human-generated ground truth for training machine-learning models. Ground truth volumes are generated by extracting small (~1 cubic micron) cuboids (called "crops") of raw FIB-SEM data and assigning a semantic label (represented as an integer ID) to every voxel in the crop. Only a few datasets were used as sources of ground truth. For those datasets that have ground truth data on OpenOrganelle, we currently combine all the crops  into a single logical volume, which can be found (in multiresolution form) at the following (schematized) path:   
 `s3://janelia-cosem/<dataset_name>/<dataset_name>.n5/labels/gt`  
 
This bucket (`janelia-cosem`) uses representations of data that are optimized for web-based visualization. Those interested in accessing crops as individual volumes can find those volumes in a different bucket: [s3://janelia-cosem-publications/heinrich-2021a](https://open.quiltdata.com/b/janelia-cosem-publications/tree/heinrich-2021a/)
 


### Image metadata
For N5 datasets, the `attributes.json` file for each dataset contains an object called `transform` which specifies the axis names, units, translation and scaling for that dataset. E.g., 
```json
"transform": {
        "axes": ["z", "y", "x"],
        "scale": [167.68, 128.0, 128.0],
        "translate": [81.22, 62.0, 62.0],
        "units": ["nm", "nm", "nm"]
    }
```

Note: the order of these values assumes C-ordered access to the underlying array. When attempting to parse this object in a context where the axis order is not guaranteed to be C-ordered, the axis names must be used to assign the spatial metadata to the correct axes. 

Wherever possible images are stored at multiple scales, which means that a single logical image volume is represented by multiple N5 datasets inside an N5 group. The `attributes.json` file inside that N5 group contains the metadata describing the multiscale image representation. We use an elaborated implementation of the [ome-zarr multiscale specification](https://github.com/zarr-developers/zarr-specs/issues/50). Specifically, the group-level `attributes.json` file contains a `multiscales` array that specifies the multiscale volumes present in the N5 group. For the datasets we have on s3, there will only be one multiscale volume per N5 group and thus the `multiscales` object looks like this: 

```json
"multiscales": [
        {
            "datasets": [
                {
                    "path": "s0",
                    "transform": {
                        "axes": ["z", "y", "x"],
                        "scale": [5.24, 4.0, 4.0],
                        "translate": [0.0, 0.0, 0.0],
                        "units": ["nm", "nm", "nm"]
                    }
                },
                {
                    "path": "s1",
                    "transform": {
                        "axes": ["y", "z", "x"],
                        "scale": [10.48, 8.0, 8.0],
                        "translate": [2.62, 2.0, 2.0],
                        "units": ["nm", "nm", "nm"]
                    }
                },
                {
                    "path": "s2",
                    "transform": {
                        "axes": ["z", "y", "x"],
                        "scale": [20.96, 16.0, 16.0],
                        "translate": [7.86, 6.0, 6.0],
                        "units": ["nm", "nm", "nm"]
                    }
                },
                {
                    "path": "s3",
                    "transform": {
                        "axes": ["z", "y", "x"],
                        "scale": [41.92, 32.0, 32.0],
                        "translate": [18.34, 14.0, 14.0],
                        "units": ["nm", "nm", "nm"]
                    }
                },
                {
                    "path": "s4",
                    "transform": {
                        "axes": ["z", "y", "x"],
                        "scale": [83.84, 64.0, 64.0],
                        "translate": [39.300000000000004, 30.0, 30.0],
                        "units": ["nm", "nm", "nm"]
                    }
                },
                {
                    "path": "s5",
                    "transform": {
                        "axes": ["z", "y", "x"],
                        "scale": [167.68, 128.0, 128.0 ],
                        "translate": [81.22, 62.0, 62.0],
                        "units": ["nm", "nm", "nm"]
                    }
                }
            ]
        }
    ],
```
As can be seen, the multiscale volume is specified by a list of objects with a `path` property (denoting the location of the volume), and a `transform` property (identical to the `transform` property in the respective array metadata).  


For images stored in neuroglancer precomputed format, see the respective [documentation](https://github.com/google/neuroglancer/blob/master/src/neuroglancer/datasource/precomputed/volume.md) of that format for details.
